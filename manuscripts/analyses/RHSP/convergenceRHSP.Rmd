
---
title: "Analyzing the convergence of the RHSP" 
author: "Michael Koch"
date: "07-03-2022"
output: html_document
---


# Step1: Prepearations

```{r, message = F, warning = F}
# load packages outside of simulation
library(papaja)
#source required functions & parameters
source('~/1vs2StepBayesianRegSEM/R/functions.R')
source('~/1vs2StepBayesianRegSEM/R/parameters.R')
```

```{r, cache = T}
# load results
resultsRHSP <- read.csv("~/OneDrive/ms/thesis/output/resultsRHSP.csv", 
                        sep = " ", 
                        header = TRUE)
convRHSP <- read.csv("~/OneDrive/ms/thesis/output/convRHSP.csv",
                     sep = " ",
                     header = TRUE)
```

```{r}
# inspect a bit
nrow(resultsRHSP)
```

```{r}
(nrow(condPop)*nrow(condRHSP)*nIter) - nrow(resultsRHSP)
```

122 chains failed, which is in correspondence with the error R gave, when running the simulation on the server. Let 's find out the positions of those that failed. Perhaps they can be re-run. If they cannot, this may also be a result. 

```{r}
comp <- 1:76800
failedChainIndex <- which((comp %in% resultsRHSP$pos) == FALSE)
length(failedChainIndex)
failedChainIndex
```

These indicices are all from the first 200 iterations. We can simply find the set of conditions belong to this by subsetting the first row of the results object.

```{r}
resultsRHSP %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, iteration) %>%
  head(1)
```

Thus, with N = 100, cross = 0.2 and the hyper-parameters all on their lowest value (0.1 for scales, and 1 for df's) the chains don't converge in the majority of the cases.

These are 156 rows. Interestingly, the chains that did not fail show good convergence, which is sort of puzzling, but maybe not?

I will however, remove them from the analysis, as there are too few iterations for a reliable picture of this set of conditions.

```{r}
resultsRHSP <- 
  resultsRHSP %>% 
  filter(pos > 200)
```

```{r}
convRHSP <- 
  convRHSP %>% 
  filter(pos > 200)
```


# Convergence

Let's explore the convergence a bit (of the remaining set of conditions).

## Divergent transitions

```{r}
totalSumDiv <- sum(convRHSP$sumDiv)/2
totalSumDiv
totalSumDiv / (2000*nrow(convRHSP)/2)
```

In total, there are a gazillion divergent transition, but only a fraction of all transitions were divergent. Let's start by computing the percentage per individual replication. And then we can check how big that is on average per set of conditions.

```{r}
convRHSP %>% 
  filter(parameter == "Rhat") %>%
  select(sumDiv) %>% 
  max()
```


```{r}
meanPropDiv <- 
convRHSP %>% 
  filter(parameter == "Rhat") %>% # doesn't matter which you pick here
  mutate(propDiv = sumDiv/ (samplePars$nSampling + samplePars$nWarmup)) %>% 
  group_by(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross) %>%
  summarise(meanPropDiv = mean(propDiv))
meanPropDiv
```

On average, they appear to be quite low. There appears to be no set of conditions where the proportion of divergent transition is lager than 0.09. 

Let's find the indices of the runs where the divergent transitions exceeded 5%.

```{r}
indicesDIVHigher5Perc <- 
convRHSP %>% 
  filter(parameter == "Rhat") %>% # doesn't matter which you pick here
  mutate(propDIV = sumDiv/(samplePars$nSampling + samplePars$nWarmup)) %>% 
  filter(propDIV >= convCriteria$strict$maxPropDIV
) %>% 
  select(pos) %>% 
  pull()
length(indicesDIVHigher5Perc)
```

This would remove 2770 rows.

## The conditions under which this happens (on average!!!, FF met Sara checken dit) are:

```{r}
meanPropDIVlarger5 <- 
convRHSP %>% 
  filter(parameter == "Rhat") %>% # doesn't matter which you pick here
  mutate(propDiv = sumDiv / (samplePars$nSampling + samplePars$nWarmup)) %>% 
  group_by(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross) %>%
  summarise(meanPropDiv = round(mean(propDiv), 2),
            maxPropDiv = max(propDiv)) %>% 
  filter(meanPropDiv > .05) %>% 
  mutate(scaleGlobal = round(scaleGlobal, 1),
         scaleLocal = round(scaleLocal, 1),
         scaleSlab = round(scaleSlab, 1), 
         cross = round(cross, 1))

# make colnames nice
colnames(meanPropDIVlarger5)[1] <- "$s_{global}$"
colnames(meanPropDIVlarger5)[2] <- "$df_{global}$"
colnames(meanPropDIVlarger5)[3] <- "$s_{local}$"
colnames(meanPropDIVlarger5)[4] <- "$df_{local}$"
colnames(meanPropDIVlarger5)[5] <- "$s_{slab}$"
colnames(meanPropDIVlarger5)[6] <- "$df_{slab}$"
colnames(meanPropDIVlarger5)[8] <- "Size $\\lambda_{c1 , 6}$"
colnames(meanPropDIVlarger5)[9] <- "Mean"
colnames(meanPropDIVlarger5)[10] <- "Max"

meanPropDIVlarger5
```

```{r}
readr::write_rds(meanPropDIVlarger5, file = "~/1vs2StepBayesianRegSEM/Rmd/figures/meanPropDIVlarger5.Rds")
```

## Effective Sample Size

Let's find the indices of all transitions where the effective sample size is less than 10% of the chain-length.

```{r}
indicesNEffTooSmall <- 
convRHSP %>% 
  filter(parameter == "n_eff") %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, pos, lambdaMainC.1.:theta.6.) %>% 
  mutate_at(.vars = vars(-(scaleGlobal:pos)), .funs = function(x){x/samplePars$nSampling}) %>% 
  filter_at(.vars = vars(lambdaMainC.1.:theta.6.), ~.x < convCriteria$strict$minPropNEFF) %>% 
  select(pos) %>% 
  pull()
```

```{r}
length(indicesNEffTooSmall)
```

By this you would lose 542 rows, which does not seem so bad.


### R-Hat

Let's find the indices of all transitions where the R-hat is smaller than 1.05.

```{r}
indicesRHatTooSmall <- 
convRHSP %>% 
  filter(parameter == "Rhat") %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, pos, lambdaMainC.1.:theta.6.) %>% 
  # get indices of all iterations where for any parameter 
  filter_at(.vars = vars(lambdaMainC.1.:theta.6.), ~.x >= convCriteria$strict$maxRhat) %>% 
  select(pos) %>% 
  pull()
```


```{r}
length(indicesRHatTooSmall)
```

This technically removes 15 rows...

```{r}
indicesCombined <- unique(c(indicesRHatTooSmall, indicesNEffTooSmall
                            #, indicesDIVTooHigh
                            ))
length(indicesCombined)
```

.. but they were all part of the rows with too little n_samp. 

It's interesting to check, where we lost the rows specifically (under what conditions).

```{r}
resultsRHSP %>% 
  filter(pos %in% indicesCombined) %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, pos)
```

## Table Conditions Removed rows

Let's make a table summarizing the rows that we removed based on Rhat and Neff.

```{r}
rowsRemovedLarger5Perc <- 
convRHSP %>% 
  filter(parameter == "n_eff") %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, pos, lambdaMainC.1.:theta.6.) %>%
  mutate_at(.vars = vars(-(scaleGlobal:pos)), .funs = function(x){x/samplePars$nSampling}) %>% 
  filter_at(.vars = vars(lambdaMainC.1.:theta.6.), ~.x < convCriteria$strict$minPropNEFF) %>% 
  count(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross) %>% 
  arrange(n) %>% 
  filter(n >= 0.05*nIter)
# make colnames nice
colnames(rowsRemovedLarger5Perc)[1] <- "$s_{global}$"
colnames(rowsRemovedLarger5Perc)[2] <- "$df_{global}$"
colnames(rowsRemovedLarger5Perc)[3] <- "$s_{local}$"
colnames(rowsRemovedLarger5Perc)[4] <- "$df_{local}$"
colnames(rowsRemovedLarger5Perc)[5] <- "$s_{slab}$"
colnames(rowsRemovedLarger5Perc)[6] <- "$df_{slab}$"
colnames(rowsRemovedLarger5Perc)[8] <- "Size $\\lambda_{c1 , 6}$"
colnames(rowsRemovedLarger5Perc)[9] <- "N removed Rep."

rowsRemovedLarger5Perc 
```

```{r}
readr::write_rds(rowsRemovedLarger5Perc, file = "~/1vs2StepBayesianRegSEM/Rmd/figures/rowsRemovedLarger5Perc.Rds")
```

# Post-process

Here, I can subset the data based on the indices accquired above.
Next, I arrange the data based on conditions and iterations and recaculate the bias of the cross-loadings, which went wrong. 


```{r}
resultsRHSPFiltered <- 
  resultsRHSP %>% 
  filter(!(pos%in% indicesCombined)) %>% 
  arrange(scaleGlobal, scaleLocal, dfGlobal, dfLocal, nu, scaleSlab, N, cross, iteration) %>%  
  mutate(biasCrossMean_1 =abs(crossEstMean_1 - cross),
         biasCrossMean_2 = abs(crossEstMean_2 - 0),
         biasCrossMean_3 = abs(crossEstMean_3 - 0),
         biasCrossMean_4 = abs(crossEstMean_4 - 0),
         biasCrossMean_5 = abs(crossEstMean_5 - 0),
         biasCrossMean_6 = abs(crossEstMean_6 - cross))
nrow(resultsRHSPFiltered) + length(indicesCombined)
# Save filtered data as .Rds file for further analysis
saveRDS(resultsRHSPFiltered, file = "~/1vs2StepBayesianRegSEM/output/resultsRHSPFilter.Rds")
```

```{r}
nrow(resultsRHSPFiltered)
```

# Making a selection of one combination of conditions

It's also useful for presenting results to make a choice of one configuration of conditions. N needs to be 100 to match the SVNP, and for the rest it might be smart to chose all hyper-parameters on 1. Let's see, how the convergence is in this case:

```{r}
#indicesNEffTooSmallAllOne <- 
convRHSP %>% 
  filter(parameter == "n_eff") %>%
  filter(scaleGlobal == 1 & dfGlobal == 1 & scaleLocal == 1 & dfLocal == 1 & scaleSlab == 1 & nu == 1) %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, pos, lambdaMainC.1.:theta.6.) %>% 
  mutate_at(.vars = vars(-(scaleGlobal:pos)), .funs = function(x){x/samplePars$nSampling}) %>% 
  filter_at(.vars = vars(lambdaMainC.1.:theta.6.), ~.x < .90) 
```

1 replication where the effiective sample size is smaller than 10% of the chain-length (is automatically removed trough the filtering step). Only 7 rows where it's smaller than 50% , 24 75%, 36 90%.

## Rhat
```{r}
#indicesRHatTooSmallAllOne <- 
convRHSP %>% 
  filter(parameter == "Rhat") %>% 
  filter(scaleGlobal == 1 & dfGlobal == 1 & scaleLocal == 1 & dfLocal == 1 & scaleSlab == 1 & nu == 1 & N == 100) %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, pos, lambdaMainC.1.:theta.6.) %>% 
  # get indices of all iterations where for any parameter 
  filter_at(.vars = vars(lambdaMainC.1.:theta.6.), ~.x >= convCriteria$strict$maxRhat) 
```

Not a single row where Rhat >= 1.05

```{r}
#indicesDIVHigher5PercAllOne <- 
convRHSP %>% 
  filter(parameter == "Rhat") %>% # doesn't matter which you pick here
  filter(scaleGlobal == 1 & dfGlobal == 1 & scaleLocal == 1 & dfLocal == 1 & scaleSlab == 1 & nu == 1 & N == 100) %>% 
  mutate(propDIV = sumDiv/ samplePars$nSampling) %>% 
  filter(propDIV >= convCriteria$strict$maxPropDIV)  
```

13 rows where percentage div transitions > 5% (not removed).

These conditions, which are a logical default, appear to work well for presenting results. Let's therefore save this subset of the results for further analysis. 




