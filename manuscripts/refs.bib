
@article{van_erp_shrinkage_2019,
	title = {Shrinkage priors for {Bayesian} penalized regression},
	volume = {89},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249618300567},
	doi = {10.1016/j.jmp.2018.12.004},
	abstract = {In linear regression problems with many predictors, penalized regression techniques are often used to guard against overfitting and to select variables relevant for predicting an outcome variable. Recently, Bayesian penalization is becoming increasingly popular in which the prior distribution performs a function similar to that of the penalty term in classical penalization. Specifically, the so-called shrinkage priors in Bayesian penalization aim to shrink small effects to zero while maintaining true large effects. Compared to classical penalization techniques, Bayesian penalization techniques perform similarly or sometimes even better, and they offer additional advantages such as readily available uncertainty estimates, automatic estimation of the penalty parameter, and more flexibility in terms of penalties that can be considered. However, many different shrinkage priors exist and the available, often quite technical, literature primarily focuses on presenting one shrinkage prior and often provides comparisons with only one or two other shrinkage priors. This can make it difficult for researchers to navigate through the many prior options and choose a shrinkage prior for the problem at hand. Therefore, the aim of this paper is to provide a comprehensive overview of the literature on Bayesian penalization. We provide a theoretical and conceptual comparison of nine different shrinkage priors and parametrize the priors, if possible, in terms of scale mixture of normal distributions to facilitate comparisons. We illustrate different characteristics and behaviors of the shrinkage priors and compare their performance in terms of prediction and variable selection in a simulation study. Additionally, we provide two empirical examples to illustrate the application of Bayesian penalization. Finally, an R package bayesreg is available online (https://github.com/sara-vanerp/bayesreg) which allows researchers to perform Bayesian penalized regression with novel shrinkage priors in an easy manner.},
	language = {en},
	urldate = {2021-06-03},
	journal = {Journal of Mathematical Psychology},
	author = {Van Erp, Sara and Oberski, Daniel L. and Mulder, Joris},
	month = apr,
	year = {2019},
	pages = {31--50},
	file = {van Erp et al. - 2019 - Shrinkage priors for Bayesian penalized regression.pdf:/home/michi/Zotero/storage/4S7YATAQ/van Erp et al. - 2019 - Shrinkage priors for Bayesian penalized regression.pdf:application/pdf},
}

@article{jacobucci_regularized_2016,
	title = {Regularized {Structural} {Equation} {Modeling}},
	volume = {23},
	issn = {1070-5511, 1532-8007},
	url = {http://www.tandfonline.com/doi/full/10.1080/10705511.2016.1154793},
	doi = {10.1080/10705511.2016.1154793},
	abstract = {A new method is proposed that extends the use of regularization in both lasso and ridge regression to structural equation models. The method is termed regularized structural equation modeling (RegSEM). RegSEM penalizes speciﬁc parameters in structural equation models, with the goal of creating easier to understand and simpler models. Although regularization has gained wide adoption in regression, very little has transferred to models with latent variables. By adding penalties to speciﬁc parameters in a structural equation model, researchers have a high level of ﬂexibility in reducing model complexity, overcoming poor ﬁtting models, and the creation of models that are more likely to generalize to new samples. The proposed method was evaluated through a simulation study, two illustrative examples involving a measurement model, and one empirical example involving the structural part of the model to demonstrate RegSEM’s utility.},
	language = {en},
	number = {4},
	urldate = {2021-06-03},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Jacobucci, Ross and Grimm, Kevin J. and McArdle, John J.},
	month = jul,
	year = {2016},
	pages = {555--566},
	file = {Jacobucci et al. - 2016 - Regularized Structural Equation Modeling.pdf:/home/michi/Zotero/storage/SFDUNLZN/Jacobucci et al. - 2016 - Regularized Structural Equation Modeling.pdf:application/pdf},
}

@article{muthen_bayesian_2012,
	title = {Bayesian {SEM}: {A} more ﬂexible representation of substantive theory},
	doi = {10.1037/a0026802},
	language = {en},
	author = {Muthén, Bengt and Asparouhov, Tihomir},
	year = {2012},
	pages = {78},
	file = {Muthen and Asparouhov - Bayesian SEM A more ﬂexible representation of sub.pdf:/home/michi/Zotero/storage/2YF49DGA/Muthen and Asparouhov - Bayesian SEM A more ﬂexible representation of sub.pdf:application/pdf},
}

@article{lu_bayesian_2016,
	title = {Bayesian {Factor} {Analysis} as a {Variable}-{Selection} {Problem}: {Alternative} {Priors} and {Consequences}},
	volume = {51},
	issn = {0027-3171, 1532-7906},
	shorttitle = {Bayesian {Factor} {Analysis} as a {Variable}-{Selection} {Problem}},
	url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2016.1168279},
	doi = {10.1080/00273171.2016.1168279},
	abstract = {Factor analysis is a popular statistical technique for multivariate data analysis. Developments in the structural equation modeling framework have enabled the use of hybrid confirmatory/exploratory approaches in which factor-loading structures can be explored relatively flexibly within a confirmatory factor analysis (CFA) framework. Recently, Muthén \& Asparouhov proposed a Bayesian structural equation modeling (BSEM) approach to explore the presence of cross loadings in CFA models. We show that the issue of determining factor-loading patterns may be formulated as a Bayesian variable selection problem in which Muthén and Asparouhov’s approach can be regarded as a BSEM approach with ridge regression prior (BSEM-RP). We propose another Bayesian approach, denoted herein as the Bayesian structural equation modeling with spike-and-slab prior (BSEM-SSP), which serves as a one-stage alternative to the BSEM-RP. We review the theoretical advantages and disadvantages of both approaches and compare their empirical performance relative to two modification indices-based approaches and exploratory factor analysis with target rotation. A teacher stress scale data set is used to demonstrate our approach.},
	language = {en},
	number = {4},
	urldate = {2021-09-07},
	journal = {Multivariate Behavioral Research},
	author = {Lu, Zhao-Hua and Chow, Sy-Miin and Loken, Eric},
	month = jul,
	year = {2016},
	pages = {519--539},
	file = {Lu et al. - 2016 - Bayesian Factor Analysis as a Variable-Selection P.pdf:/home/michi/Zotero/storage/B6H9T4QI/Lu et al. - 2016 - Bayesian Factor Analysis as a Variable-Selection P.pdf:application/pdf},
}

@article{park_bayesian_2008,
	title = {The {Bayesian} {Lasso}},
	volume = {103},
	issn = {0162-1459, 1537-274X},
	doi = {10.1198/016214508000000337},
	language = {en},
	number = {482},
	urldate = {2021-09-08},
	journal = {Journal of the American Statistical Association},
	author = {Park, Trevor and Casella, George},
	month = jun,
	year = {2008},
	pages = {681--686},
	file = {Park and Casella - 2008 - The Bayesian Lasso.pdf:/home/michi/Zotero/storage/63LSCEKS/Park and Casella - 2008 - The Bayesian Lasso.pdf:application/pdf},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso}},
	volume = {58},
	issn = {2517-6161},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x},
	doi = {10.1111/j.2517-6161.1996.tb02080.x},
	abstract = {We propose a new method for estimation in linear models. The ‘lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	language = {en},
	number = {1},
	urldate = {2021-09-10},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1996.tb02080.x},
	keywords = {quadratic programming, regression, shrinkage, subset selection},
	pages = {267--288},
	file = {Snapshot:/home/michi/Zotero/storage/7WXZE2SW/j.2517-6161.1996.tb02080.html:text/html;Full Text PDF:/home/michi/Zotero/storage/2JLTWQ3R/Tibshirani - 1996 - Regression Shrinkage and Selection Via the Lasso.pdf:application/pdf},
}

@article{carpenter_stan_2017,
	title = {Stan: {A} {Probabilistic} {Programming} {Language}},
	volume = {76},
	copyright = {Copyright (c) 2017 Bob Carpenter, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, Allen Riddell},
	issn = {1548-7660},
	shorttitle = {Stan},
	doi = {10.18637/jss.v076.i01},
	abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
	language = {en},
	number = {1},
	urldate = {2021-09-10},
	journal = {Journal of Statistical Software},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	month = jan,
	year = {2017},
	note = {Number: 1},
	keywords = {algorithmic differentiation, Bayesian inference, probabilistic programming, Stan},
	pages = {1--32},
	file = {Full Text:/home/michi/Zotero/storage/7NPIICKB/Carpenter et al. - 2017 - Stan A Probabilistic Programming Language.pdf:application/pdf},
}

@article{merkle_efficient_2020,
	title = {Efficient {Bayesian} {Structural} {Equation} {Modeling} in {Stan}},
	url = {http://arxiv.org/abs/2008.07733},
	abstract = {Structural equation models comprise a large class of popular statistical models, including factor analysis models, certain mixed models, and extensions thereof. Model estimation is complicated by the fact that we typically have multiple interdependent response variables and multiple latent variables (which may also be called random effects or hidden variables), often leading to slow and inefficient MCMC samples. In this paper, we describe and illustrate a general, efficient approach to Bayesian SEM estimation in Stan, contrasting it with previous implementations in R package blavaan (Merkle \& Rosseel, 2018). After describing the approaches in detail, we conduct a practical comparison under multiple scenarios. The comparisons show that the new approach is clearly better. We also discuss ways that the approach may be extended to other models that are of interest to psychometricians.},
	urldate = {2021-09-10},
	journal = {arXiv:2008.07733 [stat]},
	author = {Merkle, Edgar C. and Fitzsimmons, Ellen and Uanhoro, James and Goodrich, Ben},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.07733},
	keywords = {Statistics - Computation},
	annote = {Comment: 21 pages, 5 figures},
	file = {arXiv Fulltext PDF:/home/michi/Zotero/storage/XVTSVW7E/Merkle et al. - 2020 - Efficient Bayesian Structural Equation Modeling in.pdf:application/pdf;arXiv.org Snapshot:/home/michi/Zotero/storage/5MTICTCA/2008.html:text/html},
}

@article{tibshirani_regression_2011,
	title = {Regression shrinkage and selection via the lasso: a retrospective},
	volume = {73},
	issn = {1369-7412},
	shorttitle = {Regression shrinkage and selection via the lasso},
	url = {https://www.jstor.org/stable/41262671},
	abstract = {In the paper I give a brief review of the basic idea and some history and then discuss some developments since the original paper on regression shrinkage and selection via the lasso.},
	number = {3},
	urldate = {2021-09-16},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Tibshirani, Robert},
	year = {2011},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {273--282},
}

@article{hsiang_bayesian_1975,
	title = {A {Bayesian} {View} on {Ridge} {Regression}},
	volume = {24},
	issn = {0039-0526},
	url = {https://www.jstor.org/stable/2987923},
	doi = {10.2307/2987923},
	number = {4},
	urldate = {2021-09-16},
	journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
	author = {Hsiang, T. C.},
	year = {1975},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {267--268},
	file = {JSTOR Full Text PDF:/home/michi/Zotero/storage/4J2U4JX8/Hsiang - 1975 - A Bayesian View on Ridge Regression.pdf:application/pdf},
}

@article{hoerl_ridge_2000,
	title = {Ridge {Regression}: {Biased} {Estimation} for {Nonorthogonal} {Problems}},
	volume = {42},
	issn = {0040-1706},
	shorttitle = {Ridge {Regression}},
	url = {https://www.jstor.org/stable/1271436},
	doi = {10.2307/1271436},
	abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
	number = {1},
	urldate = {2021-09-16},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	year = {2000},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	pages = {80--86},
	file = {JSTOR Full Text PDF:/home/michi/Zotero/storage/N63Z3K8Z/Hoerl and Kennard - 2000 - Ridge Regression Biased Estimation for Nonorthogo.pdf:application/pdf},
}

@article{zhang_criteria_2021,
	title = {Criteria for {Parameter} {Identification} in {Bayesian} {Lasso} {Methods} for {Covariance} {Analysis}: {Comparing} {Rules} for {Thresholding}, \textit{p} -value, and {Credible} {Interval}},
	issn = {1070-5511, 1532-8007},
	shorttitle = {Criteria for {Parameter} {Identification} in {Bayesian} {Lasso} {Methods} for {Covariance} {Analysis}},
	url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2021.1945456},
	doi = {10.1080/10705511.2021.1945456},
	abstract = {The lasso is a commonly used regularization method that is increasing used in structural equation models (SEMs). Under the Bayesian framework, lasso is rendered more flexible and readily produces estimates of standard errors and the penalty parameter. However, in practice, it remains unclear what decision rule is appropriate for parameter identification; in other words, determining what size estimate is large enough to be included into the model. The current study compared three decision rules for parameter identifica­ tion – thresholding, p-value, and credible interval in confirmatory factor analysis. Specifically, two distinct parameter spaces were studied: cross-loadings and residual correlations. Results showed that the thresh­ olding rule performed best in balancing power and Type I error rate. Different thresholds for standardized estimates were needed for different conditions. Guidelines for parameter identification and recom­ mended thresholding values were also provided. Results of the current study have the potential to extend to a broad range of SEMs.},
	language = {en},
	urldate = {2021-09-16},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Zhang, Lijin and Pan, Junhao and Ip, Edward Haksing},
	month = aug,
	year = {2021},
	pages = {1--10},
	file = {Zhang et al. - 2021 - Criteria for Parameter Identification in Bayesian .pdf:/home/michi/Zotero/storage/FYK98DKJ/Zhang et al. - 2021 - Criteria for Parameter Identification in Bayesian .pdf:application/pdf},
}

@article{carvalho_horseshoe_2010,
	title = {The horseshoe estimator for sparse signals},
	volume = {97},
	issn = {0006-3444},
	doi = {10.1093/biomet/asq017},
	abstract = {This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator's advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator's tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior.},
	number = {2},
	urldate = {2021-09-16},
	journal = {Biometrika},
	author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	year = {2010},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {465--480},
	file = {JSTOR Full Text PDF:/home/michi/Zotero/storage/2VUZWZNY/CARVALHO et al. - 2010 - The horseshoe estimator for sparse signals.pdf:application/pdf},
}

@article{piironen_sparsity_2017,
	title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
	volume = {11},
	doi = {10.1214/17-EJS1337SI},
	number = {2},
	journal = {Electronic Journal of Statistics},
	author = {Piironen, Juho and Vehtari, Aki},
	year = {2017},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	pages = {5018--5051},
	file = {Full Text:/home/michi/Zotero/storage/DMBT7QEE/Piironen and Vehtari - 2017 - Sparsity information and regularization in the hor.pdf:application/pdf;Snapshot:/home/michi/Zotero/storage/9THVXYJ3/17-EJS1337SI.html:text/html},
}

@book{schoot_small_2020,
	address = {London New York},
	series = {European {Association} of {Methodology} series},
	title = {Small sample size solutions: a guide for applied researchers and practitioners\$fedited by {Rens} van de {Schoot} and {Milica} {Miočević}},
	isbn = {978-0-367-22189-8 978-0-367-22222-2},
	shorttitle = {Small sample size solutions},
	language = {en},
	publisher = {Routledge},
	editor = {Schoot, Rens van de and Miočević, Milica},
	year = {2020},
	file = {Schoot and Miočević - 2020 - Small sample size solutions a guide for applied r.pdf:/home/michi/Zotero/storage/H24PDG4V/Schoot and Miočević - 2020 - Small sample size solutions a guide for applied r.pdf:application/pdf},
}

@misc{stan_development_team_stan_2021,
	title = {Stan {User} {Guide}},
	url = {https://mc-stan.org/docs/2_27/stan-users-guide-2_27.pdf},
	urldate = {2021-09-24},
	author = {{Stan Development Team}},
	year = {2021},
	file = {stan-users-guide-2_27.pdf:/home/michi/Zotero/storage/CKR2FGXS/stan-users-guide-2_27.pdf:application/pdf},
}

@article{ghosh_use_2018,
	title = {On the {Use} of {Cauchy} {Prior} {Distributions} for {Bayesian} {Logistic} {Regression}},
	volume = {13},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-2/On-the-Use-of-Cauchy-Prior-Distributions-for-Bayesian-Logistic/10.1214/17-BA1051.full},
	doi = {10.1214/17-BA1051},
	abstract = {In logistic regression, separation occurs when a linear combination of the predictors can perfectly classify part or all of the observations in the sample, and as a result, finite maximum likelihood estimates of the regression coefficients do not exist. Gelman et al. (2008) recommended independent Cauchy distributions as default priors for the regression coefficients in logistic regression, even in the case of separation, and reported posterior modes in their analyses. As the mean does not exist for the Cauchy prior, a natural question is whether the posterior means of the regression coefficients exist under separation. We prove theorems that provide necessary and sufficient conditions for the existence of posterior means under independent Cauchy priors for the logit link and a general family of link functions, including the probit link. We also study the existence of posterior means under multivariate Cauchy priors. For full Bayesian inference, we develop a Gibbs sampler based on Pólya-Gamma data augmentation to sample from the posterior distribution under independent Student-t priors including Cauchy priors, and provide a companion R package tglm, available at CRAN. We demonstrate empirically that even when the posterior means of the regression coefficients exist under separation, the magnitude of the posterior samples for Cauchy priors may be unusually large, and the corresponding Gibbs sampler shows extremely slow mixing. While alternative algorithms such as the No-U-Turn Sampler (NUTS) in Stan can greatly improve mixing, in order to resolve the issue of extremely heavy tailed posteriors for Cauchy priors under separation, one would need to consider lighter tailed priors such as normal priors or Student-t priors with degrees of freedom larger than one.},
	number = {2},
	urldate = {2021-10-02},
	journal = {Bayesian Analysis},
	author = {Ghosh, Joyee and Li, Yingbo and Mitra, Robin},
	month = jun,
	year = {2018},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {binary regression, existence of posterior mean, Markov chain Monte Carlo, probit regression, separation, slow mixing},
	pages = {359--383},
	file = {Full Text PDF:/home/michi/Zotero/storage/A9NGHXVS/Ghosh et al. - 2018 - On the Use of Cauchy Prior Distributions for Bayes.pdf:application/pdf;Snapshot:/home/michi/Zotero/storage/EM6XRLTT/17-BA1051.html:text/html},
}

@article{betancourt_conceptual_2018,
	title = {A {Conceptual} {Introduction} to {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1701.02434},
	abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
	urldate = {2021-10-05},
	journal = {arXiv:1701.02434 [stat]},
	author = {Betancourt, Michael},
	month = jul,
	year = {2018},
	note = {arXiv: 1701.02434},
	keywords = {Statistics - Methodology},
	annote = {Comment: 60 pages, 42 figures},
	file = {arXiv Fulltext PDF:/home/michi/Zotero/storage/P694JZY7/Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Car.pdf:application/pdf;arXiv.org Snapshot:/home/michi/Zotero/storage/88ZVPIH5/1701.html:text/html},
}

@article{homan_no-u-turn_2014,
	title = {The {No}-{U}-turn sampler: adaptively setting path lengths in {Hamiltonian} {Monte} {Carlo}},
	volume = {15},
	issn = {1532-4435},
	shorttitle = {The {No}-{U}-turn sampler},
	abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size ε and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more effciently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter ε on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" samplers.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Homan, Matthew D. and Gelman, Andrew},
	month = jan,
	year = {2014},
	keywords = {Bayesian inference, Markov chain Monte Carlo, adaptive Monte Carlo, dual averaging, Hamiltonian Monte Carlo},
	pages = {1593--1623},
	file = {Full Text PDF:/home/michi/Zotero/storage/D2L7CXTP/Homan and Gelman - 2014 - The No-U-turn sampler adaptively setting path len.pdf:application/pdf},
}

@inproceedings{carvalho_handling_2009,
	title = {Handling {Sparsity} via the {Horseshoe}},
	url = {https://proceedings.mlr.press/v5/carvalho09a.html},
	abstract = {This paper presents a general, fully Bayesian framework for sparse supervised-learning problems based on the horseshoe prior. The horseshoe prior is a member of the family of multivariate scale mixtures of normals, and is therefore closely related to widely used approaches for sparse Bayesian learning, including, among others, Laplacian priors (e.g. the LASSO) and Student-t priors (e.g. the relevance vector machine). The advantages of the horseshoe are its robustness at handling unknown sparsity and large outlying signals. These properties are justifed theoretically via a representation theorem and accompanied by comprehensive empirical experiments that compare its performance to benchmark alternatives.},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Proceedings of the {Twelth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	month = apr,
	year = {2009},
	note = {ISSN: 1938-7228},
	pages = {73--80},
	file = {Full Text PDF:/home/michi/Zotero/storage/YCAXJV9K/Carvalho et al. - 2009 - Handling Sparsity via the Horseshoe.pdf:application/pdf},
}

@article{piironen_projection_2015,
	title = {Projection predictive variable selection using {Stan}+{R}},
	url = {http://arxiv.org/abs/1508.02502},
	abstract = {This document is additional material to our previous study comparing several strategies for variable subset selection. Our recommended approach was to fit the full model with all the candidate variables and best possible prior information, and perform the variable selection using the projection predictive framework. Here we give an example of performing such an analysis, using Stan for fitting the model, and R for the variable selection.},
	urldate = {2021-10-18},
	journal = {arXiv:1508.02502 [stat]},
	author = {Piironen, Juho and Vehtari, Aki},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.02502},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:/home/michi/Zotero/storage/QAU35K5F/1508.html:text/html;arXiv Fulltext PDF:/home/michi/Zotero/storage/5QIPPCHN/Piironen and Vehtari - 2015 - Projection predictive variable selection using Sta.pdf:application/pdf},
}

@misc{noauthor_apa_nodate,
	title = {{APA} {PsycNet}},
	url = {https://psycnet.apa.org/doiLanding?doi=10.1037/0033-2909.111.3.490},
	urldate = {2021-10-14},
}

@article{maccallum_model_1992,
	title = {Model modifications in covariance structure analysis: the problem of capitalization on chance},
	volume = {111},
	issn = {0033-2909},
	shorttitle = {Model modifications in covariance structure analysis},
	doi = {10.1037/0033-2909.111.3.490},
	abstract = {In applications of covariance structure modeling in which an initial model does not fit sample data well, it has become common practice to modify that model to improve its fit. Because this process is data driven, it is inherently susceptible to capitalization on chance characteristics of the data, thus raising the question of whether model modifications generalize to other samples or to the population. This issue is discussed in detail and is explored empirically through sampling studies using 2 large sets of data. Results demonstrate that over repeated samples, model modifications may be very inconsistent and cross-validation results may behave erratically. These findings lead to skepticism about generalizability of models resulting from data-driven modifications of an initial model. The use of alternative a priori models is recommended as a preferred strategy.},
	language = {eng},
	number = {3},
	journal = {Psychological Bulletin},
	author = {MacCallum, R. C. and Roznowski, M. and Necowitz, L. B.},
	month = may,
	year = {1992},
	pmid = {16250105},
	keywords = {Adolescent, Algorithms, Data Interpretation, Statistical, Empirical Research, Factor Analysis, Statistical, Humans, Intelligence Tests, Job Satisfaction, Models, Psychological, Models, Statistical, Reproducibility of Results, Sample Size, Sampling Studies, Software, Surveys and Questionnaires},
	pages = {490--504},
}

@article{morris_using_2019,
	title = {Using simulation studies to evaluate statistical methods},
	volume = {38},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8086},
	doi = {10.1002/sim.8086},
	abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
	language = {en},
	number = {11},
	urldate = {2021-10-13},
	journal = {Statistics in Medicine},
	author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8086},
	keywords = {graphics for simulation, Monte Carlo, simulation design, simulation reporting, simulation studies},
	pages = {2074--2102},
	file = {Snapshot:/home/michi/Zotero/storage/GLNUHHTL/sim.html:text/html;Full Text PDF:/home/michi/Zotero/storage/A2VSNI46/Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf:application/pdf},
}

@misc{noauthor_comparison_nodate,
	title = {A {Comparison} of {Bayesian} and {Frequentist} {Model} {Selection} {Methods} for {Factor} {Analysis} {Models}},
	url = {https://oce-ovid-com.proxy.library.uu.nl/article/00060744-201706000-00009/HTML},
	urldate = {2021-11-17},
	file = {A Comparison of Bayesian and Frequentist Model Selection Methods for Factor Analysis Models:/home/michi/Zotero/storage/XTR3F955/HTML.html:text/html},
}

@article{monnahan_faster_2017,
	title = {Faster estimation of {Bayesian} models in ecology using {Hamiltonian} {Monte} {Carlo}},
	volume = {8},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12681},
	doi = {10.1111/2041-210X.12681},
	abstract = {Bayesian inference is a powerful tool to better understand ecological processes across varied subfields in ecology, and is often implemented in generic and flexible software packages such as the widely used BUGS family (BUGS, WinBUGS, OpenBUGS and JAGS). However, some models have prohibitively long run times when implemented in BUGS. A relatively new software platform called Stan uses Hamiltonian Monte Carlo (HMC), a family of Markov chain Monte Carlo (MCMC) algorithms which promise improved efficiency and faster inference relative to those used by BUGS. Stan is gaining traction in many fields as an alternative to BUGS, but adoption has been slow in ecology, likely due in part to the complex nature of HMC. Here, we provide an intuitive illustration of the principles of HMC on a set of simple models. We then compared the relative efficiency of BUGS and Stan using population ecology models that vary in size and complexity. For hierarchical models, we also investigated the effect of an alternative parameterization of random effects, known as non-centering. For small, simple models there is little practical difference between the two platforms, but Stan outperforms BUGS as model size and complexity grows. Stan also performs well for hierarchical models, but is more sensitive to model parameterization than BUGS. Stan may also be more robust to biased inference caused by pathologies, because it produces diagnostic warnings where BUGS provides none. Disadvantages of Stan include an inability to use discrete parameters, more complex diagnostics and a greater requirement for hands-on tuning. Given these results, Stan is a valuable tool for many ecologists utilizing Bayesian inference, particularly for problems where BUGS is prohibitively slow. As such, Stan can extend the boundaries of feasible models for applied problems, leading to better understanding of ecological processes. Fields that would likely benefit include estimation of individual and population growth rates, meta-analyses and cross-system comparisons and spatiotemporal models.},
	language = {en},
	number = {3},
	urldate = {2021-12-14},
	journal = {Methods in Ecology and Evolution},
	author = {Monnahan, Cole C. and Thorson, James T. and Branch, Trevor A.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12681},
	keywords = {Bayesian inference, Stan, Markov chain Monte Carlo, hierarchical modelling, no-U-turn sampler},
	pages = {339--348},
	file = {Snapshot:/home/michi/Zotero/storage/XP2RFGP6/2041-210X.html:text/html;Full Text PDF:/home/michi/Zotero/storage/LHX3AWCW/Monnahan et al. - 2017 - Faster estimation of Bayesian models in ecology us.pdf:application/pdf},
}

@article{van_erp_prior_2018,
	title = {Prior sensitivity analysis in default {Bayesian} structural equation modeling},
	volume = {23},
	issn = {1939-1463},
	doi = {10.1037/met0000162},
	abstract = {Bayesian structural equation modeling (BSEM) has recently gained popularity because it enables researchers to fit complex models and solve some of the issues often encountered in classical maximum likelihood estimation, such as nonconvergence and inadmissible solutions. An important component of any Bayesian analysis is the prior distribution of the unknown model parameters. Often, researchers rely on default priors, which are constructed in an automatic fashion without requiring substantive prior information. However, the prior can have a serious influence on the estimation of the model parameters, which affects the mean squared error, bias, coverage rates, and quantiles of the estimates. In this article, we investigate the performance of three different default priors: noninformative improper priors, vague proper priors, and empirical Bayes priors—with the latter being novel in the BSEM literature. Based on a simulation study, we find that these three default BSEM methods may perform very differently, especially with small samples. A careful prior sensitivity analysis is therefore needed when performing a default BSEM analysis. For this purpose, we provide a practical step-by-step guide for practitioners to conducting a prior sensitivity analysis in default BSEM. Our recommendations are illustrated using a well-known case study from the structural equation modeling literature, and all code for conducting the prior sensitivity analysis is available in the online supplemental materials. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
	number = {2},
	journal = {Psychological Methods},
	author = {van Erp, Sara and Mulder, Joris and Oberski, Daniel L.},
	year = {2018},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Statistical Analysis, Statistical Probability, Structural Equation Modeling},
	pages = {363--388},
	file = {Snapshot:/home/michi/Zotero/storage/DBYTKIUU/2017-52406-001.html:text/html;Submitted Version:/home/michi/Zotero/storage/DTLS28IZ/van Erp et al. - 2018 - Prior sensitivity analysis in default Bayesian str.pdf:application/pdf},
}

@book{cox_principles_2006,
	title = {Principles of {Statistical} {Inference}},
	isbn = {978-1-139-45913-6},
	abstract = {In this definitive book, D. R. Cox gives a comprehensive and balanced appraisal of statistical inference. He develops the key concepts, describing and comparing the main ideas and controversies over foundational issues that have been keenly argued for more than two-hundred years. Continuing a sixty-year career of major contributions to statistical thought, no one is better placed to give this much-needed account of the field. An appendix gives a more personal assessment of the merits of different ideas. The content ranges from the traditional to the contemporary. While specific applications are not treated, the book is strongly motivated by applications across the sciences and associated technologies. The mathematics is kept as elementary as feasible, though previous knowledge of statistics is assumed. The book will be valued by every user or student of statistics who is serious about understanding the uncertainty inherent in conclusions from statistical analyses.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Cox, D. R.},
	month = aug,
	year = {2006},
	note = {Google-Books-ID: nRgtGZXi2KkC},
	keywords = {Business \& Economics / Statistics, Mathematics / Probability \& Statistics / General, Medical / Epidemiology, Social Science / Research},
}

@book{bollen_structural_1989,
	title = {Structural {Equations} with {Latent} {Variables}},
	isbn = {978-0-471-01171-2},
	abstract = {Analysis of Ordinal Categorical Data Alan Agresti Statistical Science Now has its first coordinated manual of methods for analyzing ordered categorical data. This book discusses specialized models that, unlike standard methods underlying nominal categorical data, efficiently use the information on ordering. It begins with an introduction to basic descriptive and inferential methods for categorical data, and then gives thorough coverage of the most current developments, such as loglinear and logit models for ordinal data. Special emphasis is placed on interpretation and application of methods and contains an integrated comparison of the available strategies for analyzing ordinal data. This is a case study work with illuminating examples taken from across the wide spectrum of ordinal categorical applications. 1984 (0 471-89055-3) 287 pp. Regression Diagnostics Identifying Influential Data and Sources of Collinearity David A. Belsley, Edwin Kuh and Roy E. Welsch This book provides the practicing statistician and econometrician with new tools for assessing the quality and reliability of regression estimates. Diagnostic techniques are developed that aid in the systematic location of data points that are either unusual or inordinately influential; measure the presence and intensity of collinear relations among the regression data and help to identify the variables involved in each; and pinpoint the estimated coefficients that are potentially most adversely affected. The primary emphasis of these contributions is on diagnostics, but suggestions for remedial action are given and illustrated. 1980 (0 471-05856-4) 292 pp. Applied Regression Analysis Second Edition Norman Draper and Harry Smith Featuring a significant expansion of material reflecting recent advances, here is a complete and up-to-date introduction to the fundamentals of regression analysis, focusing on understanding the latest concepts and applications of these methods. The authors thoroughly explore the fitting and checking of both linear and nonlinear regression models, using small or large data sets and pocket or high-speed computing equipment. Features added to this Second Edition include the practical implications of linear regression; the Durbin-Watson test for serial correlation; families of transformations; inverse, ridge, latent root and robust regression; and nonlinear growth models. Includes many new exercises and worked examples. 1981 (0 471-02995-5) 709 pp.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Bollen, Kenneth A.},
	month = may,
	year = {1989},
	note = {Google-Books-ID: 4a3UDwAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@book{james_introduction_2021,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {An {Introduction} to {Statistical} {Learning}: with {Applications} in {R}},
	isbn = {978-1-07-161417-4 978-1-07-161418-1},
	shorttitle = {An {Introduction} to {Statistical} {Learning}},
	url = {https://link.springer.com/10.1007/978-1-0716-1418-1},
	language = {en},
	urldate = {2022-04-21},
	publisher = {Springer US},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2021},
	doi = {10.1007/978-1-0716-1418-1},
	file = {James et al. - 2021 - An Introduction to Statistical Learning with Appl.pdf:/home/michi/Zotero/storage/R52MEIUV/James et al. - 2021 - An Introduction to Statistical Learning with Appl.pdf:application/pdf},
}

@article{ishwaran_spike_2005,
	title = {Spike and slab variable selection: {Frequentist} and {Bayesian} strategies},
	volume = {33},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Spike and slab variable selection},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-33/issue-2/Spike-and-slab-variable-selection-Frequentist-and-Bayesian-strategies/10.1214/009053604000001147.full},
	doi = {10.1214/009053604000001147},
	abstract = {Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure’s ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty.},
	number = {2},
	urldate = {2022-04-21},
	journal = {The Annals of Statistics},
	author = {Ishwaran, Hemant and Rao, J. Sunil},
	month = apr,
	year = {2005},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {shrinkage, 62J05, 62J07, Generalized ridge regression, hypervariance, model averaging, model uncertainty, ordinary least squares, Penalization, rescaling, stochastic variable selection, Zcut},
	pages = {730--773},
	file = {Snapshot:/home/michi/Zotero/storage/D3EDH79C/009053604000001147.html:text/html;Full Text PDF:/home/michi/Zotero/storage/VERMAD96/Ishwaran and Rao - 2005 - Spike and slab variable selection Frequentist and.pdf:application/pdf},
}

@article{mitchell_bayesian_1988,
	title = {Bayesian {Variable} {Selection} in {Linear} {Regression}},
	volume = {83},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2290129},
	doi = {10.2307/2290129},
	abstract = {This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a "spike and slab" distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation σ, where ln(σ) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter γ, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing γ, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against γ are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose γ. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of "leave one out" approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods.},
	number = {404},
	urldate = {2022-04-21},
	journal = {Journal of the American Statistical Association},
	author = {Mitchell, T. J. and Beauchamp, J. J.},
	year = {1988},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1023--1032},
}

@article{george_variable_1993,
	title = {Variable {Selection} {Via} {Gibbs} {Sampling}},
	volume = {88},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2290777},
	doi = {10.2307/2290777},
	abstract = {A crucial problem in building a multiple regression model is the selection of predictors to include. The main thrust of this article is to propose and develop a procedure that uses probabilistic considerations for selecting promising subsets. This procedure entails embedding the regression setup in a hierarchical normal mixture model where latent variables are used to identify subset choices. In this framework the promising subsets of predictors can be identified as those with higher posterior probability. The computational burden is then alleviated by using the Gibbs sampler to indirectly sample from this multinomial posterior distribution on the set of possible subset choices. Those subsets with higher probability--the promising ones--can then be identified by their more frequent appearance in the Gibbs sample.},
	number = {423},
	urldate = {2022-04-21},
	journal = {Journal of the American Statistical Association},
	author = {George, Edward I. and McCulloch, Robert E.},
	year = {1993},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {881--889},
}

@inproceedings{piironen_hyperprior_2017,
	title = {On the {Hyperprior} {Choice} for the {Global} {Shrinkage} {Parameter} in the {Horseshoe} {Prior}},
	url = {https://proceedings.mlr.press/v54/piironen17a.html},
	abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but as shown in this paper, the results can be sensitive to the prior choice for the global shrinkage hyperparameter. We argue that the previous default choices are dubious due to their tendency to favor solutions with more unshrunk coefficients than we typically expect a priori. This can lead to bad results if this parameter is not strongly identified by data. We derive the relationship between the global parameter and the effective number of nonzeros in the coefficient vector, and show an easy and intuitive way of setting up the prior for the global parameter based on our prior beliefs about the number of nonzero coefficients in the model. The results on real world data show that one can benefit greatly – in terms of improved parameter estimates, prediction accuracy, and reduced computation time – from transforming even a crude guess for the number of nonzero coefficients into the prior for the global parameter using our framework.},
	language = {en},
	urldate = {2022-04-23},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Piironen, Juho and Vehtari, Aki},
	month = apr,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {905--913},
	file = {Full Text PDF:/home/michi/Zotero/storage/P6LSMW7Y/Piironen and Vehtari - 2017 - On the Hyperprior Choice for the Global Shrinkage .pdf:application/pdf;Supplementary PDF:/home/michi/Zotero/storage/KIGACJBM/Piironen and Vehtari - 2017 - On the Hyperprior Choice for the Global Shrinkage .pdf:application/pdf},
}

@article{hastie_statistical_2015,
	title = {Statistical learning with sparsity},
	volume = {143},
	journal = {Monographs on statistics and applied probability},
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	year = {2015},
	pages = {143},
	file = {Full Text:/home/michi/Zotero/storage/NJXGEJIM/Hastie et al. - 2015 - Statistical learning with sparsity.pdf:application/pdf},
}

@article{obrien_statistical_2016,
	title = {Statistical {Learning} with {Sparsity}: {The} {Lasso} and {Generalizations}: {Book} {Reviews}},
	volume = {84},
	issn = {03067734},
	shorttitle = {Statistical {Learning} with {Sparsity}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/insr.12167},
	doi = {10.1111/insr.12167},
	language = {en},
	number = {1},
	urldate = {2022-05-02},
	journal = {International Statistical Review},
	author = {O'Brien, Carl M.},
	month = apr,
	year = {2016},
	pages = {156--157},
	file = {O'Brien - 2016 - Statistical Learning with Sparsity The Lasso and .pdf:/home/michi/Zotero/storage/5UHSC2U4/O'Brien - 2016 - Statistical Learning with Sparsity The Lasso and .pdf:application/pdf},
}

@article{yuan_which_2021,
	title = {Which {Method} is {More} {Reliable} in {Performing} {Model} {Modification}: {Lasso} {Regularization} or {Lagrange} {Multiplier} {Test}?},
	volume = {28},
	issn = {1070-5511, 1532-8007},
	shorttitle = {Which {Method} is {More} {Reliable} in {Performing} {Model} {Modification}},
	url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2020.1768858},
	doi = {10.1080/10705511.2020.1768858},
	abstract = {Data-driven model modification plays an important role for a statistical methodology to advance the understanding of subjective matters. However, when the sample size is not sufficiently large model modification using the Lagrange multiplier (LM) test has been found not performing well due to capitalization on chance. With the recent development of lasso regression in statistical learning, lasso regularization for structural equation modeling (SEM) may seem to be a method that could avoid capitalizing on chance in finding an adequate model. But there is little evidence validating the goodness of lasso SEM. The purpose of this article is to examine the performance of lasso SEM by comparing it against the LM test, aiming to answer the following five questions: (1) Can we trust the results of lasso SEM for model modification? (2) Does the performance of lasso SEM depend more on the effect size or the absolute value of the parameter? (3) Does lasso SEM perform better than the widely used LM test for model modification? (4) Are lasso SEM and LM test affected by nonnormally distributed data in practice? and (5) Do lasso SEM and LM test perform better with robustly transformed data? By addressing these questions with real data, results indicate that lasso SEM is unable to deliver the expected promises, and it does not perform better than the LM test.},
	language = {en},
	number = {1},
	urldate = {2022-05-16},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Yuan, Ke-Hai and Liu, Fang},
	month = jan,
	year = {2021},
	pages = {69--81},
	file = {Yuan and Liu - 2021 - Which Method is More Reliable in Performing Model .pdf:/home/michi/Zotero/storage/SNTF4FND/Yuan and Liu - 2021 - Which Method is More Reliable in Performing Model .pdf:application/pdf},
}

@article{polson_shrink_2010,
	title = {Shrink globally, act locally: {Sparse} {Bayesian} regularization and prediction},
	volume = {9},
	shorttitle = {Shrink globally, act locally},
	number = {501-538},
	journal = {Bayesian statistics},
	author = {Polson, Nicholas G. and Scott, James G.},
	year = {2010},
	note = {Publisher: Citeseer},
	pages = {105},
}

@article{datta_asymptotic_2013,
	title = {Asymptotic properties of {Bayes} risk for the horseshoe prior},
	volume = {8},
	number = {1},
	journal = {Bayesian Analysis},
	author = {Datta, Jyotishka and Ghosh, Jayanta K.},
	year = {2013},
	note = {Publisher: International Society for Bayesian Analysis},
	pages = {111--132},
	file = {Full Text:/home/michi/Zotero/storage/UNYW4ZTF/Datta and Ghosh - 2013 - Asymptotic properties of Bayes risk for the horses.pdf:application/pdf},
}

@article{van_der_pas_horseshoe_2014,
	title = {The horseshoe estimator: {Posterior} concentration around nearly black vectors},
	volume = {8},
	shorttitle = {The horseshoe estimator},
	number = {2},
	journal = {Electronic Journal of Statistics},
	author = {Van Der Pas, Stéphanie L. and Kleijn, Bas JK and Van Der Vaart, Aad W.},
	year = {2014},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	pages = {2585--2618},
	file = {Full Text:/home/michi/Zotero/storage/55WP936A/Van Der Pas et al. - 2014 - The horseshoe estimator Posterior concentration a.pdf:application/pdf;Snapshot:/home/michi/Zotero/storage/Z75R7JFL/14-EJS962.html:text/html},
}

@article{r_core_team_r_2021,
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	author = {{R Core Team}},
	year = {2021},
}

@article{zhang_blcfa_2021,
	title = {blcfa: {An} {R} {Package} for {Bayesian} {Model} {Modification} in {Confirmatory} {Factor} {Analysis}},
	volume = {28},
	issn = {1070-5511},
	shorttitle = {blcfa},
	url = {https://doi.org/10.1080/10705511.2020.1867862},
	doi = {10.1080/10705511.2020.1867862},
	abstract = {In confirmatory factor analysis (CFA), post hoc model modification (PMM) indexes are often used to adjust for possible residual correlations between items. Although the approach is useful for improving model goodness-of-fit, it requires an iterative, one-item-pair-at-a-time procedure that can be tedious and prone to error. This paper provides a didactic discussion in the form of a tutorial of a more efficient and practical alternative and its implementation using an R-based package. The tutorial contains (1) the Bayesian covariance Lasso (least absolute shrinkage and selection operator) approach as an alternative to the PMM method, and (2) the R package blcfa, which implements the Bayesian covariance lasso and directly interfaces with Mplus. It adopts a two-step approach by first estimating the entire residual covariance matrix, and then identifying the nonzero entries and seamlessly feeding them into Mplus. Two examples were used to illustrate package implementation.},
	number = {4},
	urldate = {2022-05-19},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Zhang, Lijin and Pan, Junhao and Dubé, Laurette and Ip, Edward Haksing},
	month = jul,
	year = {2021},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/10705511.2020.1867862},
	keywords = {Bayesian method, blcfa package, confirmatory factor analysis, covariance Lasso},
	pages = {649--658},
	file = {Snapshot:/home/michi/Zotero/storage/MQUFRKRK/10705511.2020.html:text/html},
}

@article{edgar_merkle_blavaan_2022,
	title = {Blavaan: {Bayesian} {Latent} {Variable} {Analysis}},
	url = {https://cran.r-project.org/web/packages/blavaan/blavaan.pdf},
	author = {Edgar Merkle and {Yves Rosseel} and {Ben Goodrich} and {Mauricio Garnier-Villarreal} and {Terrence D. Jorgensen} and {Huub Hoofs} and {Rens van de Schoot} and {Andrew Johnson} and {Matthew Emery}},
	year = {2022},
}

@article{chen_jinsong_partially_2021,
	title = {A {Partially} {Confirmatory} {Approach} to {Scale} {Development} {With} the {Bayesian} {Lasso}},
	volume = {26},
	url = {https://oce-ovid-com.proxy.library.uu.nl/article/00060744-202104000-00005/HTML},
	number = {2},
	urldate = {2022-05-31},
	journal = {Psychological Methods},
	author = {Chen, Jinsong and {Guo, Zhihan} and {Zhang, Lijin} and {Pan, Junhao}},
	year = {2021},
	pages = {210--235},
	file = {A Partially Confirmatory Approach to Scale Development With the Bayesian Lasso:/home/michi/Zotero/storage/PPID6JXB/HTML.html:text/html},
}

@article{serang_exploratory_2017,
	title = {Exploratory {Mediation} {Analysis} via {Regularization}},
	volume = {24},
	issn = {1070-5511, 1532-8007},
	url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2017.1311775},
	doi = {10.1080/10705511.2017.1311775},
	abstract = {Exploratory mediation analysis refers to a class of methods used to identify a set of potential mediators of a process of interest. Despite its exploratory nature, conventional approaches are rooted in conﬁrmatory traditions, and as such have limitations in exploratory contexts. We propose a two-stage approach called exploratory mediation analysis via regularization (XMed) to better address these concerns. We demonstrate that this approach is able to correctly identify mediators more often than conventional approaches and that its estimates are unbiased. Finally, this approach is illustrated through an empirical example examining the relationship between college acceptance and enrollment.},
	language = {en},
	number = {5},
	urldate = {2022-05-31},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Serang, Sarfaraz and Jacobucci, Ross and Brimhall, Kim C. and Grimm, Kevin J.},
	month = sep,
	year = {2017},
	pages = {733--744},
	file = {Serang et al. - 2017 - Exploratory Mediation Analysis via Regularization.pdf:/home/michi/Zotero/storage/4BBLKRI4/Serang et al. - 2017 - Exploratory Mediation Analysis via Regularization.pdf:application/pdf},
}

@article{guo_bayesian_2012,
	title = {Bayesian {Lasso} for {Semiparametric} {Structural} {Equation} {Models}},
	volume = {68},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2012.01751.x},
	doi = {10.1111/j.1541-0420.2012.01751.x},
	abstract = {There has been great interest in developing nonlinear structural equation models and associated statistical inference procedures, including estimation and model selection methods. In this paper a general semiparametric structural equation model (SSEM) is developed in which the structural equation is composed of nonparametric functions of exogenous latent variables and fixed covariates on a set of latent endogenous variables. A basis representation is used to approximate these nonparametric functions in the structural equation and the Bayesian Lasso method coupled with a Markov Chain Monte Carlo (MCMC) algorithm is used for simultaneous estimation and model selection. The proposed method is illustrated using a simulation study and data from the Affective Dynamics and Individual Differences (ADID) study. Results demonstrate that our method can accurately estimate the unknown parameters and correctly identify the true underlying model.},
	language = {en},
	number = {2},
	urldate = {2022-05-31},
	journal = {Biometrics},
	author = {Guo, Ruixin and Zhu, Hongtu and Chow, Sy-Miin and Ibrahim, Joseph G.},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2012.01751.x},
	keywords = {Bayesian Lasso, Latent variable, Spline, Structural equation model},
	pages = {567--577},
	file = {Snapshot:/home/michi/Zotero/storage/XMJ2QDYF/j.1541-0420.2012.01751.html:text/html;Accepted Version:/home/michi/Zotero/storage/YD7JZ3AN/Guo et al. - 2012 - Bayesian Lasso for Semiparametric Structural Equat.pdf:application/pdf},
}

@article{hans_bayesian_2009,
	title = {Bayesian lasso regression},
	volume = {96},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/27798870},
	abstract = {The lasso estimate for linear regression corresponds to a posterior mode when independent, double-exponential prior distributions are placed on the regression coefficients. This paper introduces new aspects of the broader Bayesian treatment of lasso regression. A direct characterization of the regression coefficients' posterior distribution is provided, and computation and inference under this characterization is shown to be straightforward. Emphasis is placed on point estimation using the posterior mean, which facilitates prediction of future observations via the posterior predictive distribution. It is shown that the standard lasso prediction method does not necessarily agree with model-based, Bayesian predictions. A new Gibbs sampler for Bayesian lasso regression is introduced.},
	number = {4},
	urldate = {2022-05-31},
	journal = {Biometrika},
	author = {Hans, Chris},
	year = {2009},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {835--845},
	file = {JSTOR Full Text PDF:/home/michi/Zotero/storage/YLSXN9CF/HANS - 2009 - Bayesian lasso regression.pdf:application/pdf},
}

@article{jacobucci_practical_2019,
	title = {A {Practical} {Guide} to {Variable} {Selection} in {Structural} {Equation} {Modeling} by {Using} {Regularized} {Multiple}-{Indicators}, {Multiple}-{Causes} {Models}},
	volume = {2},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245919826527},
	doi = {10.1177/2515245919826527},
	abstract = {Methodological innovations have allowed researchers to consider increasingly sophisticated statistical models that are better in line with the complexities of real-world behavioral data. However, despite these powerful new analytic approaches, sample sizes may not always be sufficiently large to deal with the increase in model complexity. This difficult modeling scenario entails large models with a limited number of observations given the number of parameters. Here, we describe a particular strategy to overcome this challenge: regularization, a method of penalizing model complexity during estimation. Regularization has proven to be a viable option for estimating parameters in this small-sample, many-predictors setting, but so far it has been used mostly in linear regression models. We show how to integrate regularization within structural equation models, a popular analytic approach in psychology. We first describe the rationale behind regularization in regression contexts and how it can be extended to regularized structural equation modeling. We then evaluate our approach using a simulation study, showing that regularized structural equation modeling outperforms traditional structural equation modeling in situations with a large number of predictors and a small sample size. Next, we illustrate the power of this approach in two empirical examples: modeling the neural determinants of visual short-term memory and identifying demographic correlates of stress, anxiety, and depression.},
	number = {1},
	urldate = {2022-05-31},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Jacobucci, Ross and Brandmaier, Andreas M. and Kievit, Rogier A.},
	month = mar,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	pages = {55--76},
	file = {SAGE PDF Full Text:/home/michi/Zotero/storage/SYR58R52/Jacobucci et al. - 2019 - A Practical Guide to Variable Selection in Structu.pdf:application/pdf},
}

@article{jacobucci_comparison_2018,
	title = {Comparison of {Frequentist} and {Bayesian} {Regularization} in {Structural} {Equation} {Modeling}},
	volume = {25},
	issn = {1070-5511, 1532-8007},
	url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2017.1410822},
	doi = {10.1080/10705511.2017.1410822},
	abstract = {Research in regularization, as applied to structural equation modeling (SEM), remains in its infancy. Speciﬁcally, very little work has compared regularization approaches across both frequentist and Bayesian estimation. The purpose of this study was to address just that, demonstrating both similarity and distinction across estimation frameworks, while speciﬁcally highlighting more recent developments in Bayesian regularization. This is accomplished through the use of two empirical examples that demonstrate both ridge and lasso approaches across both frequentist and Bayesian estimation, along with detail regarding software implementation. We conclude with a discussion of future research, advocating for increased evaluation and synthesis across both Bayesian and frequentist frameworks.},
	language = {en},
	number = {4},
	urldate = {2022-05-31},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Jacobucci, Ross and Grimm, Kevin J.},
	month = jul,
	year = {2018},
	pages = {639--649},
	file = {Jacobucci and Grimm - 2018 - Comparison of Frequentist and Bayesian Regularizat.pdf:/home/michi/Zotero/storage/RKGSWF7K/Jacobucci and Grimm - 2018 - Comparison of Frequentist and Bayesian Regularizat.pdf:application/pdf},
}

@article{liang_regularized_2020,
	title = {Regularized {Structural} {Equation} {Modeling} to {Detect} {Measurement} {Bias}: {Evaluation} of {Lasso}, {Adaptive} {Lasso}, and {Elastic} {Net}},
	volume = {27},
	issn = {1070-5511, 1532-8007},
	shorttitle = {Regularized {Structural} {Equation} {Modeling} to {Detect} {Measurement} {Bias}},
	url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2019.1693273},
	doi = {10.1080/10705511.2019.1693273},
	abstract = {Correct detection of measurement bias could help researchers revise models or refine psychological scales. Measurement bias detection can be viewed as a variable-selection problem, in which biased items are optimally selected from a set of items. This study investigated a number of regularization methods: ridge, lasso, elastic net (enet) and adaptive lasso (alasso), in comparison with maximum likelihood estimation (MLE) for detecting various forms of measurement bias in regard to a continuous violator using restricted factor analysis. Particularly, complex structural equation models with relatively small sample sizes were the study focus. Through a simulation study and an empirical example, results indicated that the enet outperformed other methods in small samples for identifying biased items. The alasso yielded low false positive rates for non-biased items outside of a high number of biased items. MLE performed well for the overall estimation of biased items.},
	language = {en},
	number = {5},
	urldate = {2022-05-31},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Liang, Xinya and Jacobucci, Ross},
	month = sep,
	year = {2020},
	pages = {722--734},
	file = {Liang and Jacobucci - 2020 - Regularized Structural Equation Modeling to Detect.pdf:/home/michi/Zotero/storage/FYMECSWQ/Liang and Jacobucci - 2020 - Regularized Structural Equation Modeling to Detect.pdf:application/pdf},
}

@article{lindstrom_model_2020,
	title = {Model {Selection} with {Lasso} in {Multi}-group {Structural} {Equation} {Models}},
	volume = {27},
	issn = {1070-5511, 1532-8007},
	url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2019.1638262},
	doi = {10.1080/10705511.2019.1638262},
	abstract = {A Structural Equations Modeling analysis of multiple groups often involves speciﬁcation of cross-group parameter equality constraints. In this paper, we present a technique for estimating the differences and equalities in parameters between groups using L1-penalized estimation (also known as the Lasso). We present the general model formulation and provide an algorithm for estimating the parameters across a range of penalization levels and a procedure for determining the amount of penalization. We also provide two case studies, one with a model including only observed variables, and one with a model with latent variables. Further, we conduct a simulation study to investigate some properties of the method.},
	language = {en},
	number = {1},
	urldate = {2022-05-31},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Lindstrøm, Jonas Christoffer and Dahl, Fredrik A.},
	month = jan,
	year = {2020},
	pages = {33--42},
	file = {Lindstrøm and Dahl - 2020 - Model Selection with Lasso in Multi-group Structur.pdf:/home/michi/Zotero/storage/NEC9CVT2/Lindstrøm and Dahl - 2020 - Model Selection with Lasso in Multi-group Structur.pdf:application/pdf},
}

@article{muthen_bsem_nodate,
	title = {{BSEM} {Measurement} {Invariance} {Analysis}},
	language = {en},
	author = {Muthen, Bengt and Asparouhov, Tihomir},
	pages = {48},
	file = {Muthen and Asparouhov - BSEM Measurement Invariance Analysis.pdf:/home/michi/Zotero/storage/NU35CKNB/Muthen and Asparouhov - BSEM Measurement Invariance Analysis.pdf:application/pdf},
}

@misc{muthen_bsem_2013,
	title = {{BSEM} {Measurement} {Invariance} {Analysis}},
	url = {https://www.statmodel.com/examples/webnotes/webnote17.pdf},
	language = {en},
	journal = {Mplus Webntoes},
	author = {Muthen, Bengt and Asparouhov, Tihomir},
	year = {2013},
	file = {Muthen and Asparouhov - BSEM Measurement Invariance Analysis.pdf:/home/michi/Zotero/storage/H8HJY47L/Muthen and Asparouhov - BSEM Measurement Invariance Analysis.pdf:application/pdf},
}

@article{serang_exploratory_2020,
	title = {Exploratory {Mediation} {Analysis} of {Dichotomous} {Outcomes} via {Regularization}},
	volume = {55},
	issn = {0027-3171, 1532-7906},
	url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2019.1608145},
	doi = {10.1080/00273171.2019.1608145},
	abstract = {Exploratory mediation analysis via regularization, or XMed, is a recently developed technique that allows one to identify potential mediators of a process of interest. However, as currently implemented, it can only be applied to continuous outcomes. We extend this method to allow application to dichotomous outcomes, including both mediators and dependent variables. Simulation results show that XMed can achieve the same sensitivity as more conventional methods for mediation analysis such as the Sobel test, percentile bootstrap, and bias-corrected bootstrap, but in general requires only half the sample size to do so. We demonstrate the implementation of this approach using an illustrative example examining the relationship between youth behavioral/emotional problems and alcohol use.},
	language = {en},
	number = {1},
	urldate = {2022-06-01},
	journal = {Multivariate Behavioral Research},
	author = {Serang, Sarfaraz and Jacobucci, Ross},
	month = jan,
	year = {2020},
	pages = {69--86},
	file = {Serang and Jacobucci - 2020 - Exploratory Mediation Analysis of Dichotomous Outc.pdf:/home/michi/Zotero/storage/PPKH8NH9/Serang and Jacobucci - 2020 - Exploratory Mediation Analysis of Dichotomous Outc.pdf:application/pdf},
}

@article{cham_estimating_2012,
	title = {Estimating {Latent} {Variable} {Interactions} {With} {Nonnormal} {Observed} {Data}: {A} {Comparison} of {Four} {Approaches}},
	volume = {47},
	issn = {0027-3171},
	shorttitle = {Estimating {Latent} {Variable} {Interactions} {With} {Nonnormal} {Observed} {Data}},
	url = {https://doi.org/10.1080/00273171.2012.732901},
	doi = {10.1080/00273171.2012.732901},
	abstract = {A Monte Carlo simulation was conducted to investigate the robustness of 4 latent variable interaction modeling approaches (Constrained Product Indicator [CPI], Generalized Appended Product Indicator [GAPI], Unconstrained Product Indicator [UPI], and Latent Moderated Structural Equations [LMS]) under high degrees of nonnormality of the observed exogenous variables. Results showed that the CPI and LMS approaches yielded biased estimates of the interaction effect when the exogenous variables were highly nonnormal. When the violation of nonnormality was not severe (normal; symmetric with excess kurtosis {\textless} 1), the LMS approach yielded the most efficient estimates of the latent interaction effect with the highest statistical power. In highly nonnormal conditions, the GAPI and UPI approaches with maximum likelihood (ML) estimation yielded unbiased latent interaction effect estimates, with acceptable actual Type I error rates for both the Wald and likelihood ratio tests of interaction effect at N ≥ 500. An empirical example illustrated the use of the 4 approaches in testing a latent variable interaction between academic self-efficacy and positive family role models in the prediction of academic performance.},
	number = {6},
	urldate = {2022-06-02},
	journal = {Multivariate Behavioral Research},
	author = {Cham, Heining and West, Stephen G. and Ma, Yue and Aiken, Leona S.},
	month = nov,
	year = {2012},
	pmid = {23457417},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00273171.2012.732901},
	pages = {840--876},
	file = {Accepted Version:/home/michi/Zotero/storage/X249SFKE/Cham et al. - 2012 - Estimating Latent Variable Interactions With Nonno.pdf:application/pdf;Snapshot:/home/michi/Zotero/storage/KT9VSIC6/00273171.2012.html:text/html},
}

@misc{stan_development_team_rstan_2022,
	title = {Rstan: the {R} interface to {Stan}.},
	url = {https://mc-stan.org/},
	author = {{Stan Development Team}},
	year = {2022},
	annote = {R package version 2.21.5},
}

@misc{r_core_team_package_2022,
	title = {Package 'parallel'},
	url = {https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf},
	author = {{R Core Team}},
	year = {2022},
}

@misc{wickham_ggplot2_2016,
	title = {ggplot2: {Elegant} {Graphics} for {Data} {Analysis}},
	url = {https://ggplot2.tidyverse.org},
	author = {Wickham, Hadley},
	year = {2016},
}

@misc{gabry_cmdstanr_2022,
	title = {Cmdstanr.},
	url = {https://mc-stan.org/cmdstanr/},
	author = {Gabry, Jonah and Češnovar, Rok},
	year = {2022},
}

@misc{wickham_tidyr_2022,
	title = {tidyr: {Tidy} {Messy} {Data}},
	url = {https://tidyr.tidyverse.org, https://github.com/tidyverse/tidyr},
	author = {Wickham, Hadley and Girlich, Maximilian},
	year = {2022},
}

@misc{wickham_dplyr_2022,
	title = {dplyr: {A} {Grammer} of {Data} {Manipulation}.},
	url = {https://dplyr.tidyverse.org, https://github.com/tidyverse/dplyr},
	author = {Wickham, Hadley and François, Romain and Müller, Kirill},
	year = {2022},
}

@misc{aust_papaja_2022,
	title = {Papaja: {Prepare} reproducible {APA} journal articles with {R} {Markdown}.},
	url = {https://github.com/crsh/papaja},
	author = {Aust, Frederik and Barth, Marius},
	year = {2022},
}

@misc{gabry_bayesplot_2022,
	title = {bayesplot: {Plotting} for {Bayesian} {Models}},
	url = {https://mc-stan.org/bayesplot/},
	author = {Gabry, Jonah and Mahr, Tristan},
	year = {2022},
}
